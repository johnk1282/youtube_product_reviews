{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer satisfaction from YouTube comments\n",
    "\n",
    "YouTube has active and popular product review channels; however, customers' comments aren't explicitly linked to products or their satisfaction (i.e. explicit ratings) so the data is less structured and unlabeled. This analysis examines the extent to which customer satisfaction on YouTube product reviews can be inferred from a classifier trained on a large open-source dataset of Amazon product reviews.\n",
    "\n",
    "This notebook performs the analysis in a few steps:\n",
    "1. Get Amazon review data (from electronics sub-category)\n",
    "2. Preprocess text and extract features\n",
    "3. Train a simple classifier\n",
    "4. Get YouTube comments from API (comments on laptop review videos)\n",
    "5. Apply preprocessing from Step 2 and classifier from Step 3 to comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from apiclient.discovery import build\n",
    "#nltk.download('punkt')\n",
    "api_key = 'xxxxxxxxxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Amazon review data\n",
    "\n",
    "The file Electronics_5.json.gz contains ~6.7 million Amazon reviews from the electronics category. The file is subset to exclude reviews of products with less than 5 reviews or from users with less than 5 reviews (see https://nijianmo.github.io/amazon/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6739590, 12)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = {}\n",
    "g = gzip.open('input/Electronics_5.json.gz', 'rb')\n",
    "for i,l in enumerate(g):\n",
    "    out[i]=json.loads(l)\n",
    "df = pd.DataFrame.from_dict(out, orient='index')\n",
    "df['reviewText'] = df['reviewText'].astype(str)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "      <td>09 18, 1999</td>\n",
       "      <td>AAP7PPBU72QFM</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>D. C. Carrad</td>\n",
       "      <td>This is the best novel I have read in 2 or 3 y...</td>\n",
       "      <td>A star is born</td>\n",
       "      <td>937612800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 23, 2013</td>\n",
       "      <td>A2E168DTVGE6SV</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>Evy</td>\n",
       "      <td>Pages and pages of introspection, in the style...</td>\n",
       "      <td>A stream of consciousness novel</td>\n",
       "      <td>1382486400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>09 2, 2008</td>\n",
       "      <td>A1ER5AYS3FQ9O3</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Paperback'}</td>\n",
       "      <td>Kcorn</td>\n",
       "      <td>This is the kind of novel to read when you hav...</td>\n",
       "      <td>I'm a huge fan of the author and this one did ...</td>\n",
       "      <td>1220313600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>09 4, 2000</td>\n",
       "      <td>A1T17LMQABMBN5</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Caf Girl Writes</td>\n",
       "      <td>What gorgeous language! What an incredible wri...</td>\n",
       "      <td>The most beautiful book I have ever read!</td>\n",
       "      <td>968025600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>02 4, 2000</td>\n",
       "      <td>A3QHJ0FXK33OBE</td>\n",
       "      <td>0151004714</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>W. Shane Schmidt</td>\n",
       "      <td>I was taken in by reviews that compared this b...</td>\n",
       "      <td>A dissenting view--In part.</td>\n",
       "      <td>949622400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0   67      True  09 18, 1999   AAP7PPBU72QFM  0151004714   \n",
       "1      3.0    5      True  10 23, 2013  A2E168DTVGE6SV  0151004714   \n",
       "2      5.0    4     False   09 2, 2008  A1ER5AYS3FQ9O3  0151004714   \n",
       "3      5.0   13     False   09 4, 2000  A1T17LMQABMBN5  0151004714   \n",
       "4      3.0    8      True   02 4, 2000  A3QHJ0FXK33OBE  0151004714   \n",
       "\n",
       "                            style      reviewerName  \\\n",
       "0       {'Format:': ' Hardcover'}      D. C. Carrad   \n",
       "1  {'Format:': ' Kindle Edition'}               Evy   \n",
       "2       {'Format:': ' Paperback'}             Kcorn   \n",
       "3       {'Format:': ' Hardcover'}   Caf Girl Writes   \n",
       "4       {'Format:': ' Hardcover'}  W. Shane Schmidt   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This is the best novel I have read in 2 or 3 y...   \n",
       "1  Pages and pages of introspection, in the style...   \n",
       "2  This is the kind of novel to read when you hav...   \n",
       "3  What gorgeous language! What an incredible wri...   \n",
       "4  I was taken in by reviews that compared this b...   \n",
       "\n",
       "                                             summary  unixReviewTime image  \n",
       "0                                     A star is born       937612800   NaN  \n",
       "1                    A stream of consciousness novel      1382486400   NaN  \n",
       "2  I'm a huge fan of the author and this one did ...      1220313600   NaN  \n",
       "3          The most beautiful book I have ever read!       968025600   NaN  \n",
       "4                        A dissenting view--In part.       949622400   NaN  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text data\n",
    "\n",
    "Below are some functions to preprocess text data for feature extraction. Note that some of these steps are done to preserve words that might contain sentiment information in bigrams, which are extracted later on. For example, the word \"not\" is important to keep in the bigram \"not happy\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"aren't\": \"are not\",\n",
    "\"arent\": \"are not\",\n",
    "\"can't\": \"can not\",\n",
    "\"cant\": \"can not\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldve\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldnt\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"didnt\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"dont\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadnt\": \"had not\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"hasnt\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"havent\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"isnt\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"itll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldve\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldnt\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"thatd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"thats\": \"that is\",\n",
    "\"there's\": \"there is\",\n",
    "\"theres\": \"there is\",\n",
    "\"they'll\": \"they will\",\n",
    "\"theyll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"theyre\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"theyve\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"wasnt\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"werent\": \"were not\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"where'd\": \"where did\",\n",
    "\"whered\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"wheres\": \"where is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wont\": \"will not\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldve\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldnt\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"youd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"youll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"youre\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"youve\": \"you have\"\n",
    "}\n",
    "\n",
    "#function to replace contractions based on the dictionary above using regex\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "#stopwords to exclude\n",
    "sw = ['of','the','on','in','if','you','i','with','it','was','when','from','this','they','for','my','to',\n",
    "      'that','there','and','be','been','than','which','as','but','these','at','what','who','why','where',\n",
    "      'a','an','is','does','do','can','did','would','will','have','are','im','ive','weve','were','am']\n",
    "\n",
    "#function to lowercase, remove non-alphabetic characters, expand contractions, and remove stopwords\n",
    "def preprocess_sentence(sentence):\n",
    "    sent = sentence.lower().strip()\n",
    "    sent = re.sub(r'[^a-zA-Z\\s]','',sent)\n",
    "    sent = expand_contractions(sent)\n",
    "    sent = sent.rstrip().strip()\n",
    "    sent = \" \".join([x for x in sent.split() if x not in sw])\n",
    "    return sent\n",
    "\n",
    "#tokenize into sentences, process each sentence, and re-join\n",
    "def preprocess_review(review):\n",
    "    sents = sent_tokenize(review)\n",
    "    sents = [preprocess_sentence(sent) for sent in sents]\n",
    "    sents = \" . \".join(sents)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual review:\n",
      "This product is excellent I feel very safe using it on my equipment I liked it so much i purchased another one you won't be disappointed !!!!!!\n",
      "--------\n",
      "Preprocessed:\n",
      "product excellent feel very safe using equipment liked so much purchased another one not disappointed . \n"
     ]
    }
   ],
   "source": [
    "#an example of result of preprocessing\n",
    "print('Actual review:')\n",
    "print(df.reviewText.values[99996])\n",
    "print('--------')\n",
    "print('Preprocessed:')\n",
    "print(preprocess_review(df.reviewText.values[99996]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess reviews using multiprocessing\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "reviews = pool.map(preprocess_review,df['reviewText'])\n",
    "pool.close()\n",
    "pool.join()\n",
    "ratings = df['overall'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, the reviews are represented as a vector of bigram counts. I use bigrams as a simple way to preserve some context that is absent in bag-of-words models. Below, filters are applied to remove bigrams that occur in less than 0.005% of reviews or more than 50% of reviews. Ultimately, there are 5,486 unique bigrams extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = len(reviews)\n",
    "min_df=max(int(np.floor(.0005*n_reviews)),10)\n",
    "max_df=int(np.floor(.5*n_reviews))\n",
    "cv = CountVectorizer(ngram_range=(2,2),max_df=max_df,min_df=min_df)\n",
    "mat = cv.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5486"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['']+cv.get_feature_names()\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below formats the vectors of bigram counts into a tensor that can be fed to a Keras embedding layer. See the section below for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_from_count_matrix(count_mat,max_len=100):\n",
    "    #count_mat is sparse matrix of work/doc counts\n",
    "    out = np.zeros((count_mat.shape[0],max_len))\n",
    "    coo = count_mat.tocoo()\n",
    "    doc_idx = -1\n",
    "    word_idx = 0\n",
    "    for doc,word,num in zip(coo.row,coo.col,coo.data):\n",
    "        if doc != doc_idx:\n",
    "            doc_idx = doc\n",
    "            word_idx = 0\n",
    "        for _ in range(num):\n",
    "            try:\n",
    "                out[doc,word_idx] = word+1\n",
    "                word_idx += 1\n",
    "            except:\n",
    "                pass\n",
    "    return out\n",
    "X = get_tensor_from_count_matrix(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the feature extraction process, the following bigrams are produced from the same example shown above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual review:\n",
      "This product is excellent I feel very safe using it on my equipment I liked it so much i purchased another one you won't be disappointed !!!!!!\n",
      "------\n",
      "Features extracted:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['product excellent',\n",
       " 'liked so',\n",
       " 'purchased another',\n",
       " 'feel very',\n",
       " 'not disappointed',\n",
       " 'another one',\n",
       " 'one not',\n",
       " 'so much']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get bigrams from one observation of a tensor\n",
    "def get_features_from_tensor(tensor_obs):\n",
    "    return [vocab[int(x)] for x in tensor_obs if vocab[int(x)]!='']\n",
    "print('Actual review:')\n",
    "print(df.reviewText.values[99996])\n",
    "print('------')\n",
    "print('Features extracted:')\n",
    "get_features_from_tensor(X[99996])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels and partition data randomly into train/test sets. Note that for this analysis I am training a binary classifier by converting 5-star ratings into 1's and anything less than 5-star ratings into 0's. A multi-class model could be used in the future to improve results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (ratings==5)*1\n",
    "part = np.random.permutation(X.shape[0])\n",
    "n_train = int(np.floor(.6*X.shape[0]))\n",
    "X_train = X[part[:n_train]] \n",
    "X_test = X[part[n_train:]] \n",
    "Y_train = Y[part[:n_train]] \n",
    "Y_test = Y[part[n_train:]]\n",
    "ratings_train = ratings[part[:n_train]] \n",
    "ratings_test = ratings[part[n_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier\n",
    "\n",
    "This section trains a simple classifier on the processed/labeled data. The model below simply learns the polarity of bigrams (1D embedding layer) and classifies the reviews' rating (customer satisfaction) based on the average polarity of the bigrams in the review. Other more complicated models could be used to improve results (i.e. sequence models, higher dimensional embeddings, more complex classifiers); however, because the goal is to apply the classifier to a new distribution of data, I chose a more constrained network based on features that are likely to also show up in the YouTube data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 100, 1)            5486      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_23  (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 5,488\n",
      "Trainable params: 5,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,1,input_shape=X[0].shape),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(.01)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4043754 samples, validate on 2695836 samples\n",
      "Epoch 1/30\n",
      "4043754/4043754 [==============================] - 22s 5us/sample - loss: 0.5644 - acc: 0.7141 - val_loss: 0.5062 - val_acc: 0.7705\n",
      "Epoch 2/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4994 - acc: 0.7766 - val_loss: 0.4973 - val_acc: 0.7793\n",
      "Epoch 3/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4955 - acc: 0.7787 - val_loss: 0.4962 - val_acc: 0.7784\n",
      "Epoch 4/30\n",
      "4043754/4043754 [==============================] - 22s 5us/sample - loss: 0.4948 - acc: 0.7789 - val_loss: 0.4962 - val_acc: 0.7802\n",
      "Epoch 5/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4946 - acc: 0.7790 - val_loss: 0.4960 - val_acc: 0.7777\n",
      "Epoch 6/30\n",
      "4043754/4043754 [==============================] - 22s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4962 - val_acc: 0.7802\n",
      "Epoch 7/30\n",
      "4043754/4043754 [==============================] - 22s 5us/sample - loss: 0.4945 - acc: 0.7790 - val_loss: 0.4959 - val_acc: 0.7787\n",
      "Epoch 8/30\n",
      "4043754/4043754 [==============================] - 22s 5us/sample - loss: 0.4945 - acc: 0.7790 - val_loss: 0.4959 - val_acc: 0.7785\n",
      "Epoch 9/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7790 - val_loss: 0.4959 - val_acc: 0.7782\n",
      "Epoch 10/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7781\n",
      "Epoch 11/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7781\n",
      "Epoch 12/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7788\n",
      "Epoch 13/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4966 - val_acc: 0.7809\n",
      "Epoch 14/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4961 - val_acc: 0.7768\n",
      "Epoch 15/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4962 - val_acc: 0.7801\n",
      "Epoch 16/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4961 - val_acc: 0.7769\n",
      "Epoch 17/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7788 - val_loss: 0.4960 - val_acc: 0.7777\n",
      "Epoch 18/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7780\n",
      "Epoch 19/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4962 - val_acc: 0.7765\n",
      "Epoch 20/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7784\n",
      "Epoch 21/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4962 - val_acc: 0.7801\n",
      "Epoch 22/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4960 - val_acc: 0.7794\n",
      "Epoch 23/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7790 - val_loss: 0.4959 - val_acc: 0.7785\n",
      "Epoch 24/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7789 - val_loss: 0.4960 - val_acc: 0.7774\n",
      "Epoch 25/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4945 - acc: 0.7788 - val_loss: 0.4967 - val_acc: 0.7757\n",
      "Epoch 26/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7788\n",
      "Epoch 27/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4966 - val_acc: 0.7809\n",
      "Epoch 28/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4960 - val_acc: 0.7795\n",
      "Epoch 29/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7785\n",
      "Epoch 30/30\n",
      "4043754/4043754 [==============================] - 21s 5us/sample - loss: 0.4944 - acc: 0.7789 - val_loss: 0.4959 - val_acc: 0.7792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size=4096,epochs=30,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is clearly learning to embed bigrams based on polarity. Some further cleansing of bigrams could reduce noise: for example, clearly \"four stars\" is a good way to separate a \"negative\" review from a \"positive\" one but will not be in the new distribution of YouTube data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative bigrams:\n",
      "['given stars' 'only stars' 'stars instead' 'four stars'\n",
      " 'very disappointing' 'three stars' 'very disappointed' 'two stars'\n",
      " 'one star' 'buyer beware']\n",
      "---------------\n",
      "Most positive bigrams:\n",
      "['love thing' 'more pleased' 'extremely happy' 'cons none'\n",
      " 'highly recommended' 'perfect replacement' 'excellent product'\n",
      " 'awesome product' 'not happier' 'say enough']\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = np.squeeze(model.layers[0].get_weights()[0])\n",
    "classifier_layer = np.squeeze(model.layers[2].get_weights()[0])\n",
    "polarity = pd.Series(embedding_layer*classifier_layer,index=vocab).sort_values()\n",
    "print('Most negative bigrams:')\n",
    "print(polarity.index[:10].values)\n",
    "print('---------------')\n",
    "print('Most positive bigrams:')\n",
    "print(polarity.index[-10:].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is pretty good, particularly given its simplicity and the fact that negative samples are noisier by construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 489772,  476847],\n",
       "       [ 118423, 1610794]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.squeeze(np.array(model.predict(X_test)))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = np.squeeze(np.array(model.predict(X_test)))\n",
    "confusion_matrix(Y_test,(pred>.5)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get YouTube comment data\n",
    "\n",
    "The functions below download metadata and text data for the top 100 comments of the top 50 videos for a particular query. In this analysis, I get a list of recent laptop releases to use as query terms, which almost exclusively result in product review videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(query,order='relevance',publishedBefore='2020-01-01T00:00:00Z'):\n",
    "    #gets the video metadata for the top 50 YouTube videos for a particular query string\n",
    "    #order in ['relevance','date','rating','viewCount']\n",
    "    req = yt.search().list(q=query,\n",
    "                       part='snippet',\n",
    "                       type='video',\n",
    "                       maxResults=50,\n",
    "                       relevanceLanguage='en',\n",
    "                       order=order,\n",
    "                       publishedBefore=publishedBefore)\n",
    "    res = req.execute()\n",
    "    video_ids = \",\".join([vid['id']['videoId'] for vid in res['items']])\n",
    "    req = yt.videos().list(part='snippet,statistics',id=video_ids)\n",
    "    res = req.execute()    \n",
    "    out = {}\n",
    "    for i,item in enumerate(res['items']):\n",
    "        vid = item['id']\n",
    "        out[vid] = {}\n",
    "        out[vid]['query'] = query\n",
    "        out[vid]['number'] = i\n",
    "        for snippet_feature in ['publishedAt','title','description']:\n",
    "            try:\n",
    "                out[vid][snippet_feature] = item['snippet'][snippet_feature]\n",
    "            except:\n",
    "                out[vid][snippet_feature] = np.nan\n",
    "        for stat_feature in ['viewCount','likeCount','dislikeCount','commentCount']:\n",
    "            try:\n",
    "                out[vid][stat_feature] = int(item['statistics'][stat_feature])\n",
    "            except:\n",
    "                out[vid][snippet_feature] = np.nan\n",
    "        try:\n",
    "            #get data for top 100 comments for the video\n",
    "            out[vid]['comments'] = get_comment_data(vid,order=order,publishedBefore=publishedBefore)\n",
    "        except:\n",
    "            #if comments are disabled\n",
    "            out[vid]['comments'] = np.nan\n",
    "    return out\n",
    "\n",
    "def get_comment_data(video_id,order='relevance',publishedBefore='2020-01-01T00:00:00Z'):\n",
    "    #gets metadata and text data for top 100 comments of a particular YouTube video \n",
    "    req = yt.commentThreads().list(part='snippet',\n",
    "                       videoId=video_id,\n",
    "                       maxResults=100,\n",
    "                       textFormat='plainText',\n",
    "                       order='relevance')\n",
    "    res = req.execute()\n",
    "    out = {}\n",
    "    for i,item in enumerate(res['items']):\n",
    "        cid = item['snippet']['topLevelComment']['id']\n",
    "        out[cid]={}\n",
    "        out[cid]['number'] = i\n",
    "        for snippet_feature in ['publishedAt','textDisplay','likeCount']:\n",
    "            try:\n",
    "                out[cid][snippet_feature] = item['snippet']['topLevelComment']['snippet'][snippet_feature]\n",
    "            except:\n",
    "                out[cid][snippet_feature] = np.nan\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the video/comment data for a set of query terms and save as pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = build('youtube','v3',developerKey=api_key)\n",
    "#a list of recent top laptop releases\n",
    "laptops = ['HP Elite Dragonfly',\n",
    "           'Dell XPS 13 2019',\n",
    "           'Huawei MateBook 13',\n",
    "           'HP Spectre x360 2019',\n",
    "           'MacBook Pro 16-inch 2019',\n",
    "           'Alienware Area-51m',\n",
    "           'Google Pixelbook Go',\n",
    "           'Microsoft Surfact Laptop 3',\n",
    "           'Dell XPS 15 2-in-1',\n",
    "           'Dell G5 15 5590',\n",
    "           'Asus Chromebook Flip',\n",
    "           'Asus VivoBook S15',\n",
    "           'Acer Switch 3',\n",
    "           'Apple MacBook 12-inch 2017',\n",
    "           'HP Spectre Folio']\n",
    "if os.path.isfile('youtube_data.p'):\n",
    "    with open('youtube_data.p','rb') as f:\n",
    "        youtube_data = pickle.load(f)\n",
    "else:\n",
    "    youtube_data = {}\n",
    "    for query in laptops:\n",
    "        print(query)\n",
    "        youtube_data.update(get_video_data(query))\n",
    "    with open('youtube_data.p','wb') as f:\n",
    "        pickle.dump(youtube_data,f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>number</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22_AGgC7QJM</th>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-19T17:00:00.000Z</td>\n",
       "      <td>HP Elite Dragonfly Review</td>\n",
       "      <td>Lisa Gade reviews the HP Elite Dragonfly 13\" p...</td>\n",
       "      <td>161061</td>\n",
       "      <td>2468</td>\n",
       "      <td>67</td>\n",
       "      <td>238</td>\n",
       "      <td>{'UgzuQR1_JPXWidT6CAx4AaABAg': {'number': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3pDn8wPD4Ng</th>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-18T06:00:02.000Z</td>\n",
       "      <td>HP Elite Dragonfly first look: A light busines...</td>\n",
       "      <td>HP is chasing superlatives again. Last year, t...</td>\n",
       "      <td>115724</td>\n",
       "      <td>1261</td>\n",
       "      <td>70</td>\n",
       "      <td>136</td>\n",
       "      <td>{'Ugzgqec21xctbHwgbOR4AaABAg': {'number': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0muI1HMvkU</th>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-10T22:47:53.000Z</td>\n",
       "      <td>HP Elite Dragonfly Review: The Stunning Busine...</td>\n",
       "      <td>HP Elite Dragonfly:  http://tidd.ly/6ceb42b8\\n...</td>\n",
       "      <td>42230</td>\n",
       "      <td>729</td>\n",
       "      <td>36</td>\n",
       "      <td>99</td>\n",
       "      <td>{'UgwM5y_BDZFhSyn2Zwx4AaABAg': {'number': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0tmnvxjj4xY</th>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-13T19:38:37.000Z</td>\n",
       "      <td>HP Elite Dragonfly unboxing and first impressions</td>\n",
       "      <td>Unboxing and first impressions of HP's ultra-l...</td>\n",
       "      <td>5118</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'Ugxju1v7yDz-QRwwPeZ4AaABAg': {'number': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZLZsSdOknz8</th>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-09-20T19:10:30.000Z</td>\n",
       "      <td>Lew Later On The 24-Hour Dragonfly Laptop</td>\n",
       "      <td>Clip from Lew Later (Episode - iPhone 11 Teard...</td>\n",
       "      <td>30103</td>\n",
       "      <td>626</td>\n",
       "      <td>28</td>\n",
       "      <td>76</td>\n",
       "      <td>{'UgxYa67chZW-nJXlisB4AaABAg': {'number': 0, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          query number               publishedAt  \\\n",
       "22_AGgC7QJM  HP Elite Dragonfly      0  2019-12-19T17:00:00.000Z   \n",
       "3pDn8wPD4Ng  HP Elite Dragonfly      1  2019-09-18T06:00:02.000Z   \n",
       "c0muI1HMvkU  HP Elite Dragonfly      2  2019-12-10T22:47:53.000Z   \n",
       "0tmnvxjj4xY  HP Elite Dragonfly      3  2019-12-13T19:38:37.000Z   \n",
       "ZLZsSdOknz8  HP Elite Dragonfly      4  2019-09-20T19:10:30.000Z   \n",
       "\n",
       "                                                         title  \\\n",
       "22_AGgC7QJM                          HP Elite Dragonfly Review   \n",
       "3pDn8wPD4Ng  HP Elite Dragonfly first look: A light busines...   \n",
       "c0muI1HMvkU  HP Elite Dragonfly Review: The Stunning Busine...   \n",
       "0tmnvxjj4xY  HP Elite Dragonfly unboxing and first impressions   \n",
       "ZLZsSdOknz8          Lew Later On The 24-Hour Dragonfly Laptop   \n",
       "\n",
       "                                                   description viewCount  \\\n",
       "22_AGgC7QJM  Lisa Gade reviews the HP Elite Dragonfly 13\" p...    161061   \n",
       "3pDn8wPD4Ng  HP is chasing superlatives again. Last year, t...    115724   \n",
       "c0muI1HMvkU  HP Elite Dragonfly:  http://tidd.ly/6ceb42b8\\n...     42230   \n",
       "0tmnvxjj4xY  Unboxing and first impressions of HP's ultra-l...      5118   \n",
       "ZLZsSdOknz8  Clip from Lew Later (Episode - iPhone 11 Teard...     30103   \n",
       "\n",
       "            likeCount dislikeCount commentCount  \\\n",
       "22_AGgC7QJM      2468           67          238   \n",
       "3pDn8wPD4Ng      1261           70          136   \n",
       "c0muI1HMvkU       729           36           99   \n",
       "0tmnvxjj4xY        38           10           10   \n",
       "ZLZsSdOknz8       626           28           76   \n",
       "\n",
       "                                                      comments  \n",
       "22_AGgC7QJM  {'UgzuQR1_JPXWidT6CAx4AaABAg': {'number': 0, '...  \n",
       "3pDn8wPD4Ng  {'Ugzgqec21xctbHwgbOR4AaABAg': {'number': 0, '...  \n",
       "c0muI1HMvkU  {'UgwM5y_BDZFhSyn2Zwx4AaABAg': {'number': 0, '...  \n",
       "0tmnvxjj4xY  {'Ugxju1v7yDz-QRwwPeZ4AaABAg': {'number': 0, '...  \n",
       "ZLZsSdOknz8  {'UgxYa67chZW-nJXlisB4AaABAg': {'number': 0, '...  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of the structure of the video data\n",
    "pd.DataFrame(youtube_data).T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for these products (queries) is comprised of 37,603 YouTube comments. In general there are between 2,000 and 3,700 comments per product (with HP Elite Dragonfly and Acer Switch 3 as exceptions with fewer comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37603, 5)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a dataframe of comments\n",
    "comment_df = pd.DataFrame()\n",
    "for vid,data in youtube_data.items():\n",
    "    comment_dict = data['comments']\n",
    "    try:\n",
    "        toadd = pd.DataFrame(comment_dict).T\n",
    "        toadd['query'] = data['query']\n",
    "        comment_df = youtube_df.append(toadd)\n",
    "    except:\n",
    "        pass\n",
    "comment_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likeCount</th>\n",
       "      <th>number</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>query</th>\n",
       "      <th>textDisplay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UgzuQR1_JPXWidT6CAx4AaABAg</th>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-19T17:24:22.000Z</td>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>Ok when is she gonna wear a shirt i dont want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UgzfJsION2Yoixzzgm54AaABAg</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-19T23:12:20.000Z</td>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>she's not wasting any time and really nailing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UgwBlVIWFDPURgAZtLd4AaABAg</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-19T17:02:32.000Z</td>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>You've been one of my favorite YouTubers since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UgwHXy-Zd9USvgvv6DF4AaABAg</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-19T17:37:08.000Z</td>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>Upgrade pricing should be near actual cost +~1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UgzyicVlmw14jKEybnF4AaABAg</th>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-19T17:14:52.000Z</td>\n",
       "      <td>HP Elite Dragonfly</td>\n",
       "      <td>Lisa is one of the best when it comes to in-de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           likeCount number               publishedAt  \\\n",
       "UgzuQR1_JPXWidT6CAx4AaABAg       166      0  2019-12-19T17:24:22.000Z   \n",
       "UgzfJsION2Yoixzzgm54AaABAg        63      1  2019-12-19T23:12:20.000Z   \n",
       "UgwBlVIWFDPURgAZtLd4AaABAg        58      2  2019-12-19T17:02:32.000Z   \n",
       "UgwHXy-Zd9USvgvv6DF4AaABAg         7      3  2019-12-19T17:37:08.000Z   \n",
       "UgzyicVlmw14jKEybnF4AaABAg        51      4  2019-12-19T17:14:52.000Z   \n",
       "\n",
       "                                         query  \\\n",
       "UgzuQR1_JPXWidT6CAx4AaABAg  HP Elite Dragonfly   \n",
       "UgzfJsION2Yoixzzgm54AaABAg  HP Elite Dragonfly   \n",
       "UgwBlVIWFDPURgAZtLd4AaABAg  HP Elite Dragonfly   \n",
       "UgwHXy-Zd9USvgvv6DF4AaABAg  HP Elite Dragonfly   \n",
       "UgzyicVlmw14jKEybnF4AaABAg  HP Elite Dragonfly   \n",
       "\n",
       "                                                                  textDisplay  \n",
       "UgzuQR1_JPXWidT6CAx4AaABAg  Ok when is she gonna wear a shirt i dont want ...  \n",
       "UgzfJsION2Yoixzzgm54AaABAg  she's not wasting any time and really nailing ...  \n",
       "UgwBlVIWFDPURgAZtLd4AaABAg  You've been one of my favorite YouTubers since...  \n",
       "UgwHXy-Zd9USvgvv6DF4AaABAg  Upgrade pricing should be near actual cost +~1...  \n",
       "UgzyicVlmw14jKEybnF4AaABAg  Lisa is one of the best when it comes to in-de...  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments per product query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query\n",
       "HP Elite Dragonfly             789\n",
       "Acer Switch 3                  920\n",
       "Dell G5 15 5590               2009\n",
       "HP Spectre Folio              2256\n",
       "Asus VivoBook S15             2498\n",
       "Microsoft Surfact Laptop 3    2555\n",
       "Asus Chromebook Flip          2671\n",
       "Google Pixelbook Go           2706\n",
       "Dell XPS 13 2019              2765\n",
       "Apple MacBook 12-inch 2017    2800\n",
       "Dell XPS 15 2-in-1            2840\n",
       "HP Spectre x360 2019          2883\n",
       "Huawei MateBook 13            3052\n",
       "Alienware Area-51m            3201\n",
       "MacBook Pro 16-inch 2019      3658\n",
       "Name: textDisplay, dtype: int64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of comments per product query')\n",
    "comment_df.groupby('query')['textDisplay'].count().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classifier to YouTube comment data\n",
    "\n",
    "The following steps are followed to predict customer satisfaction from the YouTube comment data:\n",
    "1. Preprocess comment data in the same way as Amazon review data\n",
    "2. Extract features using the vocabulary from Amazon reviews\n",
    "3. Infer sentiment/customer satisfaction using trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comment_df.textDisplay.apply(preprocess_review)\n",
    "comments_cv = CountVectorizer(ngram_range=(2,2),vocabulary=vocab[1:])\n",
    "comments_mat = comments_cv.fit_transform(comments)\n",
    "comments_tensor = get_tensor_from_count_matrix(comments_mat)\n",
    "comment_df['pred'] = np.squeeze(np.array(model.predict(comments_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to get a sense of how well the classifier works on the new distribution of data by reading some reviews. Below it looks like the classifier is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Most \"positive\" comments:\n",
      "-------------\n",
      "\n",
      "\n",
      "own this laptop and i absolutely love it, its the best laptop i ever had in my life i so love it, and its also the first  laptop i own with a white color, it's change so much from black classic gaming laptop\n",
      "\n",
      "\n",
      "Great video man, I live in Brazil and actually got the Macbook 12 2017 with the i5 processor and 512gb ssd storage based on this review, I was leaning towards either this or the MacBook Pro base model, and I decided to go with the 12 inch because portability is really important for me, since I`m a college professor and businessman, so I carry mine around all day. All I can say about this machine is that everything you said on your review is true, it is a wonderful, lightweight and very powerful device, much more powerful than my 2014 MacBook Air, which I replaced with this one. The port situation doesn't bother me at all, I only have one adaptor which lets me charge it, plug on my 32\" monitor and keep an extra USB-A port. Thanks for the review!\n",
      "\n",
      "\n",
      "Excellent review. After a lot of research I bought the Spectre x360 15\" shown in this review and this is by far the best notebook I ever had. The performance is incredible, the display is 4K and exactly what I need to run financial applications for my business (lots of charts can fit in the screen), the keyboard is awesome, numpad, audio very good, etc. But believe it or not, what I like the most in this notebook is that it runs cool (with exception of gaming) and it's quieter than any other computer in this performance category. HP did an excellent job with the fans and I'm so happy they finally fixed a known issue with the Spectre. Most of the time I don't hear it. Only when I start some heavy CPU demanding application I will notice the fan running (for me this is around the first 30 minutes of the market opening and then I forget about the fan on this notebook). And to top all of it, the battery is amazing for a computer with this performance level. If there is something I would suggest is that HP would offer a 32GB RAM version. Not sure even why start at 8GB. We should have 2 versions 16 and 32GB, but that's just me I guess. Just wait for a HP sale and buy it, you  will not regret!\n",
      "\n",
      "\n",
      "I LOVE my C302!  It's the perfect size and, like you said, the keyboard is one of the best I've ever laid my hands on.  It travels well, great battery life, and is powerful enough for 100% of the tasks I need to do (web, streaming, word processing, reports, email).  Chrome is also a great OS - fast and so convenient especially if you rely on Docs, Sheets, Google Drive, etc.  Now that it runs Android as well, it's all cherries on top!  I'm phasing out my desktop too and connecting an Asus Chromebox to my 4K TV :)\n",
      "\n",
      "\n",
      "Hi me again, guess I'm a subscriber now he he. Since we chatted the other day I sorted most of teh criticism oh my Leica TL2 by upgrading the  firmare from v1.1 to v1.5 and WOW. I digress, this is about laptops. I absolutely love HP Spectre; all the models are fantastic. I havent had a laptop for over a decade, I have always used a maxed out Linux tower (I build them from components to  save money) but, last year I got pissed off on holiday trying to surf 3D virtual worlds on my Nexus tablet  with a bluetooth keyboard; never ever again. So I started looking and the HP Spectre range jumped out at me, there is nothing  like them, they look gorgeous. Bit of research and it seems  the technology  has got to just about  where my lifetime dream is possible; what  we used to call a life drive, your entire life on a small machine. So  I bought the 15inch for my Sister and took the plunge and bought the HP Spectre 13inch 7i 4K screen 360. Yes it costs shed loads, its more expensive than the 15inch and macbooks but, gosh it ticks all the boxes. I pick it up and it slips in my Crumpler Doozie Camera bag, add my Leica and I am off on holiday. I can charge the camera and transfer photos with USB-c. Alas its got Windows 10 (could be worse it  could be apple, behave yourself girl, ProTogs love apple, sorry I'm a geek) which I am trying to love, despite it  being a tad flakey; any trouble and I will wipe the disk and load Linux Mint which supports all the  touch stuff.  Back  home I have all mod cons; I bought the HP Elite X2 USB C-Docking Station, this powers the Laptop and feeds a 27in ASUS/B&O Monitor, a Logitech dongle with Keyboard & mouse, a Dell USB DVD drive and a Asus Xonar U7 MKII USB DAC and Headphone amplifer feeding my Cyrus III stereo amplifier and Audiolab Q speakers either side of teh screen. Of course it drives my Epson V600 flatbed film scanner. Of course I still have my 7i Linux Tower if i need some power use. Once I kicked off the useless MS software (research tells me nobody  uses MS software, most  download Chrome) I did  well matching  my tower software suite. Firefox browser  synchs through Google to my Pixel2 XL phone. For photos my  favourite GIMP and DarkTable work well and Libre Office does all the  serious stuff. So am I  impressed. Yes I absolutely love my HP  Spectre and the fact I can flip it over  into  tablet mode is a bonus.  Not  sure about  leather,I think  I prefer the polycarbonate but then, my HP is usually sitting on the desk with the lid closed  feeding the docking station (oh yes not sure about cooling  with the lid closed as I am mostly using full power so I prop the lid slightly open using a stuffed toy which  works a treat). Yes HP is expensive but, then its hardly Leica or Cyrus  level, so I think it  was worth every  penny and now I can take my digital life wherever I go; I will take that as a win ;-)\n",
      "\n",
      "\n",
      "-------------\n",
      "Most \"negative\" comments:\n",
      "-------------\n",
      "\n",
      "\n",
      "Here is my experience with the Area 51m and Dell customer service. I bought it from HIDEvolution on Amazon. They apparently tell us that \"It is rare for a product that we have quality checked to fail so quickly, but it does happen from time to time.\" They are of no help either so far. \n",
      "\n",
      "Update: I posted the same review on Dell's website and I received an email that my review is against their guidelines. So they cannot publish my review on their website. I did not use any vulgar language or gave any inaccurate information. Tactically, they sent me the email to which I cannot reply. I am going to call them up and ask which part of my review is against their guidelines. They simply don't want us to put negativity be reviews. If they moderating what reviews we are supposed to put, what's the point in having such an option? No wonder they only have positive reviews for this laptop on the Dell's website.\n",
      "\n",
      "My model: i7-9700k, RTX 2070, 16GB RAM\n",
      "\n",
      "If you are planning to do some serious work on your laptop. I would suggest stay away with this Area 51m. It is still in the first generation and I can see there are too many defects with this model. I am a doctoral candidate in mathematics education and bought this laptop a month ago thinking it will be perfect to render my video lectures, animation, and also might as well finish writing my thesis on it. I don't play a lot of games. The laptop was faulty from the day I received it. I had to reinstall Windows 10 to make it work. Now a week ago there are artifacts appearing on the laptop screen. I thought it is a driver's issue and updated it. It didn't help. I cannot even get into the windows desktop as all I can see a black screen and weird artifacts after login. I bought premium support from Alienware and contacted them via email. I told them how important this laptop for my academics but they didn't reply promptly. Their responses are days apart. They gave a set of tests to run to make sure it's not a big issue. They are just waiting time as after all those tests, it is apparent that I had a faulty GPU. I gave up and called the customer care and told them the issue and updated with run tests. Then they created a job order where I had to send the laptop to their repair center. I paid for premium support so there is a In-home repair service but they did not listen to this at all. They said there is no other option but to send the laptop to their facility. They promised me that end to end service time will be less than 5 business days. After four days, I received an email saying that there are some failed parts in the laptop and they don't have those parts right now, those parts MAY arrive in five business days. They have no idea how it is affecting my academics. I relied on this thinking it is a powerful computer. They are ignoring customers suffering because they will have to answer for packing defective parts. I searched Reddit and other forums only to be surprised that I am not the only victim here.\n",
      "\n",
      "The bottom line is : buy this laptop at your own risk and don't rely on this for serious works. If you badly thinking to buy this laptop, I would wait for the next generation of this model so that they will have some time to understand customers sufferings and ensures not to pack defective parts.\n",
      "\n",
      "Another update: I received an email saying that they may get a hold of new parts in another 4 days and if the laptop still has an issue after replacement, they are going to extend the repair time to another 10 business days. I spent more than three thousand dollars on this laptop thinking it will help with my doctoral projects. I had to spend more time in the repairing than actually work on my academics.\n",
      "\n",
      "It has been 12 days and they still have no idea when the parts will be available. They just stopped replying my email. \n",
      "\n",
      "This is completely my personal thoughts coming from my experiences and readings on online forums about the product.\n",
      "\n",
      "https://www.reddit.com/r/Alienware/comments/cfrfr4/my_personal_thoughts_on_alienware_area_51m_want/\n",
      "\n",
      "\n",
      "Here is my experience with the Area 51m and Dell customer service. I bought it from HIDEvolution on Amazon. They apparently tell us that \"It is rare for a product that we have quality checked to fail so quickly, but it does happen from time to time.\" They are of no help so far. \n",
      "\n",
      "Update: I posted the same review on Dell's website and I received an email that my review is against their guidelines. So they cannot publish my review on their website. I did not use any vulgar language or gave any inaccurate information. Tactically, they sent me the email to which I cannot reply. I am going to call them up and ask which part of my review is against their guidelines. They simply don't want us to put negativity be reviews. If they moderating what reviews we are supposed to put, what's the point in having such an option? No wonder they only have positive reviews for this laptop on the Dell's website.\n",
      "\n",
      "My model: i7-9700k, RTX 2070, 16GB RAM\n",
      "\n",
      "If you are planning to do some serious work on your laptop. I would suggest stay away with this Area 51m. It is still in the first generation and I can see there are too many defects with this model. I am a doctoral candidate in mathematics education and bought this laptop a month ago thinking it will be perfect to render my video lectures, animation, and also might as well finish writing my thesis on it. I don't play a lot of games. The laptop was faulty from the day I received it. I had to reinstall Windows 10 to make it work. Now a week ago there are artifacts appearing on the laptop screen. I thought it is a driver's issue and updated it. It didn't help. I cannot even get into the windows desktop as all I can see a black screen and weird artifacts after login. I bought premium support from Alienware and contacted them via email. I told them how important this laptop for my academics but they didn't reply promptly. Their responses are days apart. They gave a set of tests to run to make sure it's not a big issue. They are just waiting time as after all those tests, it is apparent that I had a faulty GPU. I gave up and called the customer care and told them the issue and updated with run tests. Then they created a job order where I had to send the laptop to their repair center. I paid for premium support so there is a In-home repair service but they did not listen to this at all. They said there is no other option but to send the laptop to their facility. They promised me that end to end service time will be less than 5 business days. After four days, I received an email saying that there are some failed parts in the laptop and they don't have those parts right now, those parts MAY arrive in five business days. They have no idea how it is affecting my academics. I relied on this thinking it is a powerful computer. They are ignoring customers suffering because they will have to answer for packing defective parts. I searched Reddit and other forums only to be surprised that I am not the only victim here.\n",
      "\n",
      "The bottom line is : buy this laptop at your own risk and don't rely on this for serious works. If you badly thinking to buy this laptop, I would wait for the next generation of this model so that they will have some time to understand customers sufferings and ensures not to pack defective parts.\n",
      "\n",
      "Another update: I received an email saying that they may get a hold of new parts in another 4 days and if the laptop still has an issue after replacement, they are going to extend the repair time to another 10 business days. I spent more than three thousand dollars on this laptop thinking it will help with my doctoral projects. I had to spend more time in the repairing than actually work on my academics.\n",
      "\n",
      "It has been 12 days and they still have no idea when the parts will be available. They just stopped replying my email. \n",
      "\n",
      "This is completely my personal thoughts coming from my experiences and readings on online forums about the product.\n",
      "\n",
      "https://www.reddit.com/r/Alienware/comments/cfrfr4/my_personal_thoughts_on_alienware_area_51m_want/\n",
      "\n",
      "\n",
      "Dear Sir,\n",
      "\n",
      "express service code 8954316434, service tag 44362S2, dell 7572 online purchase.\n",
      "\n",
      "IPS display is having glow zones and back-light bleed areas, display bezel sticking is coming off at left lower area( not a big issue but shows poor workmanship). keyboard is flimsy, no number pad and plastic layer surrounding keyboard is cheap looking and finger print magnet, speakers are very flat and distort at full volume, touch-pad tactile feel is poor, webcam position at bottom most place is disappointment, lastly I am unable to post genuine review in dell website.\n",
      "\n",
      "I understand that dell display repair policy only qualifies for 150 lumen ambient light office area, not for dimly lit homes. This poor quality display is justified by above policy. This is misleading as it was never mentioned in fine text at time of sale.\n",
      "\n",
      "I will never buy any dell product in future and will advice everyone i come across to not buy these products and show them my product.\n",
      "\n",
      "This mail is not about customer service, this is about poor build and low quality, high price and policy justifying the same.\n",
      "\n",
      "You may be able to replace or repair the product but waste of time for consumer anyway.\n",
      "\n",
      "Also I found out that replacement units may have same or worse problems as manufacturing is poor, repairs are not top notch and to live with these poor quality products( courtesy you tube).\n",
      "\n",
      "\n",
      "I made an expensive mistake by buying the Dell XPS 2-in-1.  I would dearly love to send it back, but it’s too late.  I have the HP Spectre X360 on order.  Your review captures most of the downside I experienced with the Dell.  One difference — I got the 4K display.  My new Spectre will have the FHD display.  I haven’t been able to get used to the MagLev keyboard.  It is difficult to use for any period of time because of the extra force you have to use to press keys.  My typing error rate is much higher.  The 4K screen is pretty, but it sucks the battery.  To get 4-hour battery life, you have to turn the brightness way down.  I can’t live with 4 hours of battery life.  \n",
      "\n",
      "The lack of a USB-A port means that almost everything you plug in has to be done via a dongle or an adapter.  There are two devices I use regularly that don’t come in USB-C variants:  Logitech Unifying Receiver for my mouse and the receiver for my headset.  Yes, I know I could use Bluetooth but it doesn’t work as well as the native plug-ins for these two frequently used devices.  I have to use an adapter which means they stick out very far.  I hate that.\n",
      "\n",
      "I travel frequently.  I have a backpack for all the things you need when you travel.  Not sure how much it weights fully loaded.  Here is my point:  Dell wanted to make the thinnest laptop in this category.  It shaved off a couple of millimeters to achieve this spec.  In the process it had to have a terrible keyboard and smaller battery.  But practically speaking, if you did a blind A/B test with my backpack loaded with the Dell + adapter vs. HP + adapter, I doubt that there would be a noticeable difference.  WHen you actually use either device, the difference is huge:  worse typing experience and worst battery life.  Yes, I know — blame the 4K for much of that.\n",
      "\n",
      "What I worry about the HP is similar:  They shaved down the formerly thick bezels and have created a stumpy-size laptop.  Will it have enough room to rest my palms?  It has a much smaller trackpad.  HP would have been wise to increase the vertical size of the display the way Dell did and give a little extra room for palm rest, trackpad size, and a little bigger battery.  But that would increase the size and weight which the marketing department wouldn’t like.  Again, I suggest using the loaded backpack test as a reference point — a couple of extra millimeters, inches, and ounces to enhance usability is a trade off that will have no practical impact on true mobility.\n",
      "\n",
      "I’m hoping the HP will make me forget about the expensive mistake I made.  I’m also hoping that in a couple of years one or both of these companies will prioritize laptop user experience over specs and will aspire to “greatest laptop”, not “thinnest” or “smallest”.\n",
      "\n",
      "\n",
      "Here is my experience with this laptop and Dell customer service. \r\n",
      "\r\n",
      "Update: I posted the same review on Dell's website and I received an email that my review is against their guidelines. So they cannot publish my review on their website. I did not use any vulgar language or gave any inaccurate information. Tactically, they sent me the email to which I cannot reply. I am going to call them up and ask which part of my review is against their guidelines. They simply don't want us to put negativity be reviews. If they moderating what reviews we are supposed to put, what's the point in having such an option? No wonder they only have positive reviews for this laptop on the Dell's website.\r\n",
      "\r\n",
      "My model: i7-9700k, RTX 2070, 16GB RAM\r\n",
      "\r\n",
      "If you are planning to do some serious work on your laptop. I would suggest stay away with this Area 51m. It is still in the first generation and I can see there are too many defects with this model. I am a doctoral candidate in mathematics education and bought this laptop a month ago thinking it will be perfect to render my video lectures, animation, and also might as well finish writing my thesis on it. I don't play a lot of games. The laptop was faulty from the day I received it. I had to reinstall Windows 10 to make it work. Now a week ago there are artifacts appearing on the laptop screen. I thought it is a driver's issue and updated it. It didn't help. I cannot even get into the windows desktop as all I can see a black screen and weird artificats after login. I bought premium support from Alienware and contacted them via email. I told them how important this laptop for my academics but they didn't reply promptly. Their responses are days apart. They gave a set of tests to run to make sure it's not a big issue. They are just waiting time as after all those tests, it is apparent that I had a faulty GPU. I gave up and called the customer care and told them the issue and updated with run tests. Then they created a job order where I had to send the laptop to their repair center. They promised me that end to end service time will be less than 5 business days. After four days, I received an email saying that there are some failed parts in the laptop and they don't have stocks of those parts right now, those parts MAY arrive in five business days. They have no idea how it is affecting my academics. I relied on this thinking it is a powerful computer.  They are ignoring customers suffering because they will have to answer for packing defective parts. I searched Reddit and other forums only to be surprised that I am not the only victim here.  \r\n",
      "\r\n",
      "The bottom line is : buy this laptop at your own risk and don't rely on this for serious works. If you badly thinking to buy this laptop, I would wait for the next generation of this model so that they will have some time to understand customers sufferings and ensures not to pack defective parts.\r\n",
      " \r\n",
      "\r\n",
      "Another update: I received an email saying that they may get a hold of new parts in another 4 days and if the laptop still has an issue after replacement, they are going to extend the repair time to another 10 business days. I spent more than three thousand dollars on this laptop thinking it will help with my doctoral projects. I had to spend more time in the repairing than actually work on my academics.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-------------')\n",
    "print('Most \"positive\" comments:')\n",
    "print('-------------')\n",
    "print('\\n')\n",
    "for comment in comment_df.sort_values(by=['pred'])['textDisplay'].values[-5:]:\n",
    "    print(comment.replace('\\n',''))\n",
    "    print('\\n')\n",
    "print('-------------')\n",
    "print('Most \"negative\" comments:')\n",
    "print('-------------')\n",
    "print('\\n')\n",
    "for comment in comment_df.sort_values(by=['pred'])['textDisplay'].values[:5]:\n",
    "    print(comment)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating to the product level, we can try to get a sense of overall YouTube-customer satisfaction for each product. Here, the MacBooks have the most positive customer satisfaction and the Spectre has the most negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query\n",
       "HP Spectre x360 2019          0.646922\n",
       "Google Pixelbook Go           0.648963\n",
       "Asus Chromebook Flip          0.649582\n",
       "HP Elite Dragonfly            0.649692\n",
       "Acer Switch 3                 0.649988\n",
       "Alienware Area-51m            0.650416\n",
       "Dell XPS 15 2-in-1            0.650839\n",
       "Dell G5 15 5590               0.651589\n",
       "Microsoft Surfact Laptop 3    0.652427\n",
       "Asus VivoBook S15             0.652914\n",
       "Dell XPS 13 2019              0.652993\n",
       "HP Spectre Folio              0.653268\n",
       "Huawei MateBook 13            0.657045\n",
       "MacBook Pro 16-inch 2019      0.658816\n",
       "Apple MacBook 12-inch 2017    0.663216\n",
       "Name: pred, dtype: float32"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df.groupby('query')['pred'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & next steps\n",
    "\n",
    "A lot can be done to improve the data quality and model used here (as discussed above). However, this gives a starting point for inferring customer satisfaction from a large unlabeled dataset of YouTube comments, where there is an active community centered around product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
